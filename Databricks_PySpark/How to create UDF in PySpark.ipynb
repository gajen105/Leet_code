{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "118ec32b-c694-4600-9ae2-96b53d2af044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(1, \"Alice\", \"Johnson\"), (2, \"Bob\", \"Smith\"), (3, \"Charlie\", \"Brown\")]\n",
    "df = spark.createDataFrame(data, [\"id\", \"first_name\", \"last_name\"])\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "732fcf75-7b19-4b7c-be52-f4e764d4774f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "def to_uppercase(name):\n",
    "    return name.upper()\n",
    "to_uppercase_udf = udf(to_uppercase)\n",
    "df = df.withColumn(\"name_upper\", to_uppercase_udf(df[\"first_name\"]))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f32cfc2-936a-415c-9d97-cda174fa8c59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def full_name(first_name, last_name):\n",
    "    return first_name + \" \" + last_name\n",
    "full_name_udf = udf(full_name)\n",
    "\n",
    "df1 = df.withColumn(\"full_name\", full_name_udf(df[\"first_name\"], df[\"last_name\"]  ))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b2ac032-14c1-4dff-af8f-7fa86accf1cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data = [(1,), (2,), (3,), (4,)]\n",
    "# df = spark.createDataFrame(data, [\"number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc20decd-8aef-4848-a8ad-6289f0e3ec57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def full_name_pandas(first_name: pd.Series, last_name: pd.Series) -> pd.Series:\n",
    "    return first_name + \" \" + last_name\n",
    "\n",
    "df2 = df.withColumn(\"full_name\", full_name_pandas(df[\"first_name\"], df[\"last_name\"]))\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33940991-431d-4be0-94a5-5ca30711caba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"CastingExample\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "data = [(\"1\", \"100.5\", \"2025-03-06\"),\n",
    "        (\"2\", \"200.8\", \"2025-03-07\"),\n",
    "        (\"3\", \"150.3\", \"2025-03-08\")]\n",
    "\n",
    "columns = [\"id\", \"price\", \"date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.printSchema()\n",
    "\n",
    "df_casted = df.withColumn(\"id\", col(\"id\").cast(\"int\")) \\\n",
    "              .withColumn(\"price\", col(\"price\").cast(\"double\")) \\\n",
    "              .withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "\n",
    "df_casted.printSchema()\n",
    "\n",
    "df_casted = df.selectExpr(\"cast(id as int)\", \"cast(price as double)\", \"cast(date as date)\")\n",
    "df_casted.printSchema()\n",
    "\n",
    "df.createOrReplaceTempView(\"temp_table\")\n",
    "df_casted = spark.sql(\"SELECT CAST(id AS INT), CAST(price AS DOUBLE), CAST(date AS DATE) FROM temp_table\")\n",
    "df_casted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93f4998d-805c-4548-b5df-00920147e871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "How to create UDF in PySpark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
