{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "907d47b9-c524-4193-baca-606c503a8827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", \"Math\", 85),\n",
    "    (1, \"Alice\", \"Science\", 90),\n",
    "    (2, \"Bob\", \"Math\", 78),\n",
    "    (2, \"Bob\", \"Science\", 75),\n",
    "    (3, \"Charlie\", \"Math\", 92),\n",
    "    (3, \"Charlie\", \"Science\", 88),\n",
    "    (4, \"Charlie\", \"Math\", 88)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"subject\", \"marks\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn(\"marks\", df[\"marks\"].cast(\"int\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a19cd3d2-aa2c-4b7d-8467-2205156beb50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max, sum\n",
    "df.groupBy(\"id\",\"name\").pivot(\"subject\").agg(max(\"marks\")).show()\n",
    "# df.groupBy(\"id\", \"name\").pivot(\"subject\").agg({\"marks\": \"max\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6221b47f-9077-47c5-b541-f36bfe830825",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def heelo():\n",
    "    print(f\"Def {sys._getframe().f_code.co_name} Completed\")\n",
    "heelo()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pivoting a DataFrame in PySpark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
